{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ebab398-f8f0-4222-b702-2c61c0acbae6",
   "metadata": {},
   "source": [
    "# 参数管理\n",
    "\n",
    "在选择了架构并设置了超参数后，我们就进入了训练阶段。\n",
    "此时，我们的目标是找到使损失函数最小化的模型参数值。\n",
    "经过训练后，我们将需要使用这些参数来做出未来的预测。\n",
    "此外，有时我们希望提取参数，以便在其他环境中复用它们，\n",
    "将模型保存下来，以便它可以在其他软件中执行，\n",
    "或者为了获得科学的理解而进行检查。\n",
    "\n",
    "之前的介绍中，我们只依靠深度学习框架来完成训练的工作，\n",
    "而忽略了操作参数的具体细节。\n",
    "本节，我们将介绍以下内容：\n",
    "\n",
    "* 访问参数，用于调试、诊断和可视化；\n",
    "* 参数初始化；\n",
    "* 在不同模型组件间共享参数。\n",
    "\n",
    "(**我们首先看一下具有单隐藏层的多层感知机。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30ae690e-838f-4e7d-acd5-65b15ec268e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2685f0-3151-46bd-885f-99f848865525",
   "metadata": {},
   "source": [
    "## **参数访问**\n",
    "\n",
    "我们从已有模型中访问参数。\n",
    "当通过`Sequential`类定义模型时，\n",
    "我们可以通过索引来访问模型的任意层。\n",
    "这就像模型是一个列表一样，每层的参数都在其属性中。\n",
    "如下所示，我们可以检查第二个全连接层的参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be2a831a-95d9-4521-8655-816cf4fc45fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.2035,  0.3365, -0.2200,  0.2043, -0.0104,  0.0059,  0.2967,  0.2995]])), ('bias', tensor([-0.2925]))])\n"
     ]
    }
   ],
   "source": [
    "print(net[2].state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4e4c42-2775-403d-ae3e-696b42889e39",
   "metadata": {},
   "source": [
    "输出的结果告诉我们一些重要的事情： 首先，这个全连接层包含两个参数，分别是该层的权重和偏置。 两者都存储为单精度浮点数（float32）。 注意，参数名称允许唯一标识每个参数，即使在包含数百个层的网络中也是如此。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58447520-e35b-4103-a0a3-b9a0e8d8ac16",
   "metadata": {},
   "source": [
    "### **目标参数**\n",
    "\n",
    "注意，每个参数都表示为参数类的一个实例。\n",
    "要对参数执行任何操作，首先我们需要访问底层的数值。\n",
    "有几种方法可以做到这一点。有些比较简单，而另一些则比较通用。\n",
    "下面的代码从第二个全连接层（即第三个神经网络层）提取偏置，\n",
    "提取后返回的是一个参数类实例，并进一步访问该参数的值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6539c3b-9c86-4575-8fa2-31fabf59ab28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([-0.2925], requires_grad=True)\n",
      "tensor([-0.2925])\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))  # nn.parameter.Parameter表示参数是可以根据梯度更新的\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c07a8a-cd73-46d5-a91c-689f1bdd4957",
   "metadata": {},
   "source": [
    "参数是复合的对象，包含值、梯度和额外信息。\n",
    "这就是我们需要显式参数值的原因。\n",
    "除了值之外，我们还可以访问每个参数的梯度。\n",
    "在上面这个网络中，由于我们还没有调用反向传播，所以参数的梯度处于初始状态。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "663aa596-402e-4537-9b48-629bc5dd56d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4d1916-4ea9-4d97-9e68-cf60921921c1",
   "metadata": {},
   "source": [
    "### **一次性访问所有参数**\n",
    "\n",
    "当我们需要对所有参数执行操作时，逐个访问它们可能会很麻烦。\n",
    "当我们处理更复杂的块（例如，嵌套块）时，情况可能会变得特别复杂，\n",
    "因为我们需要递归整个树来提取每个子块的参数。\n",
    "下面，我们将通过演示来比较访问第一个全连接层的参数和访问所有层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b57796cb-da4e-4824-8277-9f278af96415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])  # 解包列表中的元素，并将它们作为单独的参数\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d5fc0d2-9c76-486a-ab16-3aa792f73485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0245])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['2.bias'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0377096-483e-40a6-8f24-199b688092f8",
   "metadata": {},
   "source": [
    "### **从嵌套块收集参数**\n",
    "\n",
    "让我们看看，如果我们将多个块相互嵌套，参数命名约定是如何工作的。\n",
    "我们首先定义一个生成块的函数（可以说是“块工厂”），然后将这些块组合到更大的块中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dacfba5c-970d-40d8-b265-e54f505d9e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4050],\n",
       "        [0.4050]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 4),\n",
    "                        nn.ReLU())\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block{i}', block1())\n",
    "    return net\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cf669c-b523-46e3-9f05-70a357cf724b",
   "metadata": {},
   "source": [
    "我们已经设计了网络，让我们看看它是如何组织的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa4c5dbe-b0a5-440e-b140-7df910f87b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74da64d-73c2-4044-af31-d1a3926c186d",
   "metadata": {},
   "source": [
    "因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。 下面，我们访问第一个主要的块中、第二个子块的第一层的偏置项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "729be270-2b1e-40e8-83ed-b2b9aadd717e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3432,  0.3495, -0.0633,  0.0482,  0.4138,  0.0512, -0.3524,  0.2329])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570547d5-5cdf-45e6-9db0-73425d81c87a",
   "metadata": {},
   "source": [
    "## 参数初始化\n",
    "\n",
    "知道了如何访问参数后，现在我们看看如何正确地初始化参数。\n",
    "我们在 :numref:`sec_numerical_stability`中讨论了良好初始化的必要性。\n",
    "深度学习框架提供默认随机初始化，\n",
    "也允许我们创建自定义初始化方法，\n",
    "满足我们通过其他规则实现初始化权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8137c21a-a976-4d55-bbdb-5b7053452460",
   "metadata": {},
   "source": [
    "### 内置初始化\n",
    "让我们首先调用内置的初始化器。 下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量， 且将偏置参数设置为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef951654-e2bf-43cf-86f0-ebf4015050b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0147, -0.0136,  0.0006,  0.0012]), tensor(0.))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weight(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)  # 下划线表示替换\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_weight)  # apply表示逐层应用init_weight函数\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf40e41-8cd1-4a53-acab-679f01b19e8c",
   "metadata": {},
   "source": [
    "我们还可以将所有参数初始化为给定的常数，比如初始化为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09d11676-0c66-4ed4-9069-21fce8612986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weight(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_weight)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e244f67b-1c11-48d6-b221-187f9f4915ed",
   "metadata": {},
   "source": [
    "我们还可以**对某些块应用不同的初始化方法**。 例如，下面我们使用Xavier初始化方法初始化第一个神经网络层， 然后将第三个神经网络层初始化为常量值42。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14ae5dcf-6b7b-404d-a847-9c0d684d7c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5308, 0.6854, 0.6664, 0.3333])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "def xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "net[0].apply(xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e16aaf9-142d-424a-b994-27224c6644bd",
   "metadata": {},
   "source": [
    "### **自定义初始化**\n",
    "\n",
    "有时，深度学习框架没有提供我们需要的初始化方法。\n",
    "在下面的例子中，我们使用以下的分布为任意权重参数$w$定义初始化方法：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    w \\sim \\begin{cases}\n",
    "        U(5, 10) & \\text{ 可能性 } \\frac{1}{4} \\\\\n",
    "            0    & \\text{ 可能性 } \\frac{1}{2} \\\\\n",
    "        U(-10, -5) & \\text{ 可能性 } \\frac{1}{4}\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bd8d498-7e81-4285-be20-f0ebd17179f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -7.8701, -0.0000,  8.6032],\n",
       "        [-0.0000, -0.0000, -0.0000,  0.0000]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\n",
    "            \"Init\",\n",
    "            *[(name, param.shape) for name, param in m.named_parameters()][0])  # 返回的是[('weight', torch.Size([8, 4]))\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "net.apply(my_init) \n",
    "net[0].weight[:2]  # 取出第一个线性层的前两行的权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fd6608-7eec-4175-a3ab-b24b1e8a528b",
   "metadata": {},
   "source": [
    "直接设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "804febbb-17c2-48a1-8090-3537d8eee109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.0000, -6.8701,  1.0000,  9.6032])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 42\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f7515a-f8c9-44d0-98a6-1710e7d2d8a0",
   "metadata": {},
   "source": [
    "## 参数绑定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c129431e-124a-41ba-a767-fcd609910b5b",
   "metadata": {},
   "source": [
    "有时我们希望在多个层间共享参数：\n",
    "我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73eb7398-3152-413e-8a4e-64cde6c67a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), shared, nn.ReLU(), shared, \n",
    "                   nn.ReLU(), nn.Linear(8, 1))\n",
    "net(X)\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b6c4cf-a14c-499d-9550-5c35f6ed03b1",
   "metadata": {},
   "source": [
    "这个例子表明第三个和第五个神经网络层的参数是绑定的。 它们不仅值相等，而且由相同的张量表示。 因此，如果我们改变其中一个参数，另一个参数也会改变。 这里有一个问题：当参数绑定时，梯度会发生什么情况？ 答案是由于模型参数包含梯度，因此在反向传播期间第二个隐藏层 （即第三个神经网络层）和第三个隐藏层（即第五个神经网络层）的梯度会加在一起。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9a89b1-200b-41ab-a546-cd3cd14b6dda",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 我们有几种方法可以访问、初始化和绑定模型参数。\n",
    "* 我们可以使用自定义初始化方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ae9640-aa5e-46a0-9fc2-678e7883e283",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
